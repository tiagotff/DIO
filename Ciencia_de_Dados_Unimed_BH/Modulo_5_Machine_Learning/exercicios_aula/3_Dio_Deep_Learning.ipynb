{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n9ZtklSdu2Z6"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas\n",
        "import numpy as np\n",
        "import torch   # Framework\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problema: Mnist -> Problema numérico. Números escritos à mão.\n",
        "# Alguns exemplos possíveis: Devolução de produto(CEP),...tudo envolvendo números manuscritos.\n",
        "\n",
        "transform = transforms.ToTensor()  #Definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #Carrega a parte do treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)  #Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)  #Carrega a parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size = 64, shuffle = True)  #Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "TiZVCcERu9xm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver imagem extraída da base de dados\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "lp0m6_FTu94W",
        "outputId": "ff0ae69e-f32f-4770-91cb-9a82534bc3a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f431e97df10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conferindo o tamanho da imagem, para verificar o tamanho do tensor da imagem\n",
        "print(imagens[0].shape)  #Para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #Para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDz8MIOkzEHB",
        "outputId": "2d8933df-172a-4cb1-c6a3-bf9b28d5f8c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Montando a estrutura da RNA\n",
        "class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Modelo, self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, 128)  #Camada de entrada, 784 neurônios que se ligam a 128\n",
        "        self.linear2 = nn.Linear(128, 64)  #Camada interna 1, 128 neurônios que se ligam a 64\n",
        "        self.linear3 = nn.Linear(64, 10)  #Camada interna 2, 64 neurônios que se ligam a 10\n",
        "\n",
        "        #Para a camada de saída não é necessário definir nada, pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.linear1(X))  #Função de ativação da camada de entrada para a camada interna 1\n",
        "        X = F.relu(self.linear2(X))  #Função de ativação da camada interna 1 para a camada interna 2\n",
        "        X = self.linear3(X)  #Função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "        return F.log_softmax(X, dim=1)  #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "lXEpXqKkzifK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Otimizador do RNA\n",
        "# Estrutura de treinamento da RNA\n",
        "\n",
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr = 0.01, momentum=0.5) # Define a política de atualizadação dos pesos e da bias\n",
        "  inicio = time()  #timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() #Definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 #Número de Epochs que o algoritmo rodará\n",
        "  modelo.train()  #Ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0  #Inicialização da perda acumulada da eopoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1)  #Convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com a ...\n",
        "      otimizador.zero_grad()   #Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device))  #Colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device))  #Calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward()  #back propagation a partir da perda\n",
        "\n",
        "      otimizador.step()  #Atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada +=perda_instantanea.item()  #Atualização da perda acumulada\n",
        "\n",
        "\n",
        "  else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) =\", (time()-inicio)/60)\n"
      ],
      "metadata": {
        "id": "nkoC65Cs3f6k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Validação\n",
        "\n",
        "def validacao (modelo, valloader, device):\n",
        "    conta_corretas, conta_todas = 0, 0\n",
        "    for imagens, etiquetas in valloader:\n",
        "      for i in range(len(etiquetas)):\n",
        "        img = imagens [i].view(1, 784)\n",
        "        #desativar o autograd para acelerar a validação. Grafos computacionais dinêmicos tem um custo alto de processamento\n",
        "        with torch.no_grad():\n",
        "          logps = modelo(img.to(device))  #output do modelo em escala logaritmica\n",
        "\n",
        "\n",
        "        ps = torch.exp(logps)   #converte output para escala normal (lembrando que é um tensor)\n",
        "\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab))   #converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred):   #compara a previsão com o valor correto\n",
        "        conta_corretas +=1\n",
        "      conta_todas +=1\n",
        "\n",
        "    print(\"Total de imagens testadas =\", conta_todas)\n",
        "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "Qtl7PF1z7Es6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH2zf1lQ9ul1",
        "outputId": "678583a0-258b-418e-c56b-24ae20709704"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rh4WHZdr_0uM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}